{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2018/08/wtf-tf-idf.html\n",
    "\n",
    "TF-IDF, which stands for term frequency — inverse document frequency, is a scoring measure widely used in information retrieval (IR) or summarization. TF-IDF is intended to reflect how relevant a term is in a given document.\n",
    "\n",
    "The intuition behind it is that if a word occurs multiple times in a document, we should boost its relevance as it should be more meaningful than other words that appear fewer times (TF). At the same time, if a word occurs many times in a document but also along many other documents, maybe it is because this word is just a frequent word; not because it was relevant or meaningful (IDF).\n",
    "\n",
    "Defining what a “relevant word” means\n",
    "\n",
    "We can come up with a more or less subjective definition driven by our intuition: a word’s relevance is proportional to the amount of information that it gives about its context (a sentence, a document or a full dataset). That is, the most relevant words are those that would help us, as humans, to better understand a whole document without reading it all.\n",
    "\n",
    "As pointed out, relevant words are not necessarily the most frequent words since stopwords like “the”, “of” or “a” tend to occur very often in many documents.\n",
    "\n",
    "There is another caveat: if we want to summarize a document compared to a whole dataset about an specific topic (let’s say, movie reviews), there will be words (other than stopwords, like character or plot), that could occur many times in the document as well as in many other documents. These words are not useful to summarize a document because they convey little discriminating power; they say very little about what the document contains compared to the other documents.\n",
    "\n",
    "Let’s go through some examples to better illustrate how TF-IDF works.\n",
    "\n",
    " \n",
    "\n",
    "Search engine example\n",
    " \n",
    "Let’s suppose we have a database with thousands of cats descriptions and a user wants to search for furry cats, so she/he issues the query “the furry cat”. As a search engine, we have to decide which documents should be returned from our database.\n",
    "\n",
    "If we have documents that match the exact query, there is no doubt but… what if we have to decide between partial matches? For simplicity, let’s say we have to choose between these two descriptions:\n",
    "\n",
    "“the lovely cat”\n",
    "“a furry kitten”\n",
    "The first description contains 2 out of 3 words from the query and the second one matches just 1 out of 3, then we would pick the first description. How can TF-IDF help us to choose the second description instead of the first one?\n",
    "\n",
    "The TF is the same for each word, no difference here. However, we could expect that the terms “cat” and “kitten” would appear in many documents (large document frequency implies low IDF), while the term “furry” will appear in fewer documents (larger IDF). So the TF-IDF for cat & kitten has a low value whereas the TF-IDF is larger for “furry”, i.e. in our database the word “furry” has more discriminative power than “cat” or “kitten”.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "If we use the TF-IDF to weight the different words that matched the query, “furry” would be more relevant than “cat” and so we could eventually choose “the furry kitten” as the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[\"the house had a tiny little mouse\", \n",
    "\"the cat saw the mouse\", \n",
    "\"the mouse ran away from the house\", \n",
    "\"the cat finally ate the mouse\", \n",
    "\"the end of the mouse story\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(use_idf=True)\n",
    "vectors = tfidf.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ate', 'away', 'cat', 'end', 'finally', 'from', 'had', 'house', 'little', 'mouse', 'of', 'ran', 'saw', 'story', 'the', 'tiny']\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.49356209 0.39820278 0.49356209 0.23518498 0.         0.\n",
      "  0.         0.         0.23518498 0.49356209]\n",
      " [0.         0.         0.48334378 0.         0.         0.\n",
      "  0.         0.         0.         0.28547062 0.         0.\n",
      "  0.59909216 0.         0.57094124 0.        ]\n",
      " [0.         0.45709287 0.         0.         0.         0.45709287\n",
      "  0.         0.36877965 0.         0.2178072  0.         0.45709287\n",
      "  0.         0.         0.43561441 0.        ]\n",
      " [0.51392301 0.         0.41462985 0.         0.51392301 0.\n",
      "  0.         0.         0.         0.24488707 0.         0.\n",
      "  0.         0.         0.48977413 0.        ]\n",
      " [0.         0.         0.         0.49175319 0.         0.\n",
      "  0.         0.         0.         0.23432303 0.49175319 0.\n",
      "  0.         0.49175319 0.46864606 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.get_feature_names())\n",
    "print(vectors.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
