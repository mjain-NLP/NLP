{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(r\"C:\\Users\\52054867\\machine learning\\Word2Vec\\word2vec-nlp-tutorial\\labeledTrainData.tsv\\labeledTrainData.tsv\", delimiter=\"\\t\", header=0)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(r\"C:\\Users\\52054867\\machine learning\\Word2Vec\\word2vec-nlp-tutorial\\testData.tsv\\testData.tsv\", delimiter=\"\\t\", header=0)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the data using BeatufulSoap module\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review):\n",
    "    \n",
    "    ##remove html tags\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    \n",
    "    # remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "    ##convert word to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    ## make a set of stopwords as set is much faster than a list. filter all the stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stops ]\n",
    "    \n",
    "    return words\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_reviews = []\n",
    "\n",
    "for i in range ( 0, len(train_data[\"review\"]) ):\n",
    "    clean_train_reviews.append( \" \".join(review_to_wordlist(train_data[\"review\"][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorization\n",
    "The Bag of Words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. For example, consider the following two sentences:\n",
    "\n",
    "Sentence 1: \"The cat sat on the hat\"\n",
    "\n",
    "Sentence 2: \"The dog ate the cat and the hat\"\n",
    "\n",
    "From these two sentences, our vocabulary is as follows:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "To get our bags of words, we count the number of times each word occurs in each sentence. In Sentence 1, \"the\" appears twice, and \"cat\", \"sat\", \"on\", and \"hat\" each appear once, so the feature vector for Sentence 1 is:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "Sentence 1: { 2, 1, 1, 1, 1, 0, 0, 0 }\n",
    "\n",
    "Similarly, the features for Sentence 2 are: { 3, 1, 0, 0, 1, 1, 1, 1}\n",
    "\n",
    "In the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 5000 most frequent words (remembering that stop words have already been removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<25000x5000 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 1975006 stored elements in Compressed Sparse Row format>,\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer = \"word\",   \\\n",
    "                         tokenizer = None,    \\\n",
    "                         preprocessor = None, \\\n",
    "                         stop_words = None,   \\\n",
    "                         max_features = 5000)\n",
    "\n",
    "train_data_features = cv.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an\n",
    "# array\n",
    "\n",
    "np.asarray(train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with random forest\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "forest = rf.fit(train_data_features,train_data[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_review = []\n",
    "\n",
    "for i in range ( 0, len(test_data[\"review\"]) ):\n",
    "    clean_test_review.append( \" \".join(review_to_wordlist(test_data[\"review\"][i])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features = cv.fit_transform(clean_test_review)\n",
    "\n",
    "np.asarray(test_data_features)\n",
    "\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#confusion_matrix()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\The Love Letter\\\" is one of those movies that could have been really clever, but they wasted it. Focusing on a letter wreaking havoc in a small town, the movie has an all-star cast with nothing to do. Tom Selleck and Alice Drummond had so recently co-starred in the super-hilarious \\\"In & Out\\\" (also about an upset in a small town), in which they were both great, but here they look as though they're getting drug all over the place. I can't tell what the people behind the camera are trying to do here (if anything), but they sure didn't accomplish anything. How tragic, that a potential laugh riot got so sorrowfully wasted.\"\n",
      "0\n",
      "This movie is a disaster within a disaster film. It is full of great action scenes, which are only meaningful if you throw away all sense of reality. Let's see, word to the wise, lava burns you; steam burns you. You can't stand next to lava. Diverting a minor lava flow is difficult, let alone a significant one. Scares me to think that some might actually believe what they saw in this movie.<br /><br />Even worse is the significant amount of talent that went into making this film. I mean the acting is actually very good. The effects are above average. Hard to believe somebody read the scripts for this and allowed all this talent to be wasted. I guess my suggestion would be that if this movie is about to start on TV ... look away! It is like a train wreck: it is so awful that once you know what is coming, you just have to watch. Look away and spend your time on more meaningful content.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print test_data['review'][12]\n",
    "print result[12]\n",
    "\n",
    "print test_data['review'][1]\n",
    "print result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Vectorization\n",
    "If we are working with movie reviews, the word “movie” will be frequent but not useful. If we were working with email data, on the other hand, the word “movie” may not be frequent and would be useful.\n",
    "\n",
    "The simplest way to account for these overrepresented words is to divide word count by the proportion of text documents each word appeared in. For example, the document:\n",
    "\n",
    "“I loved this movie! It was great, great, great.”\n",
    "\n",
    "contains the word “loved” and “movie” once each. Now, let’s suppose that we look at all the other documents and find that, in total, “loved” appears in 1% of text documents and “movie” appears in 33%. We could now weight our scores as\n",
    "\n",
    "“loved” = times it appears in text / proportion of texts it appears in = 1 / 1% “movie” = times it appears in text / proportion of texts it appears in = 1 / 33%\n",
    "\n",
    "Before applying weights, both “loved” and “movie” had a score of 1 (since each word appeared in the sentence once). After we apply weights, “loved” has a score of 100 and “movie” has a score of 3. The score for “loved” is much higher relative to “movie”, indicating that we care about the word “loved” much more than “movie”.\n",
    "\n",
    "In fact, our score for “loved” is now 33 times larger than our score for “movie”. While we suspect that “movie” should be less important than “loved” for predicting whether a review is positive or negative, this relative difference might be too big. Very rare words — perhaps, misspelled words — will receive too much relative weight in our current weighting scheme.\n",
    "\n",
    "We need to strike a balance between downweighting very frequent words without overweighting rare words. This is what term frequency–inverse document frequency (tf-idf) weighting does for us. In the simple weighting scheme, we used the formula:\n",
    "\n",
    "times a word appears in text * (1 / proportion of texts it appears in)\n",
    "\n",
    "tf-idf weighting alters this formula slightly by taking the log of the second term:\n",
    "\n",
    "times a word appears in text * log(1 / proportion of texts it appears in)\n",
    "\n",
    "By taking the log, we ensure that our weight changes slowly in relation to how frequently a word appears in all our documents. This means that while common words are downweighted, they aren’t downweighted too much.\n",
    "\n",
    "\n",
    "\n",
    "To read more about tf-idf see this medium post: https://medium.com/civis-analytics/an-intro-to-natural-language-processing-in-python-framing-text-classification-in-familiar-terms-33778d1aa3ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('count', cv),('tfidf',TfidfTransformer())]).fit(clean_train_reviews)\n",
    "\n",
    "train_data_features = pipeline.transform(clean_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<25000x5000 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1975006 stored elements in Compressed Sparse Row format>,\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = rf.fit(train_data_features,train_data[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\The Love Letter\\\" is one of those movies that could have been really clever, but they wasted it. Focusing on a letter wreaking havoc in a small town, the movie has an all-star cast with nothing to do. Tom Selleck and Alice Drummond had so recently co-starred in the super-hilarious \\\"In & Out\\\" (also about an upset in a small town), in which they were both great, but here they look as though they're getting drug all over the place. I can't tell what the people behind the camera are trying to do here (if anything), but they sure didn't accomplish anything. How tragic, that a potential laugh riot got so sorrowfully wasted.\"\n",
      "0\n",
      "I come from Bangladesh, and here, C.C.Costigan is a goddess of awesome sex. All kidding aside, a friend and I were awake in the middle of the night, watching movies on the Encore: Action channel, when we came across a series of sci-fi-esquire flicks. There was RoboCop 2 (not bad,...not bad at all) ... then Judge Dredd, (Stalone almost ruins his career) then a movie called Lethal Target. One would think the title \\Lethal Target\\\" could only be awarded to a really cool, and really cheesy Rambo knock-off. But nay, what is delivered is what I would like to call a \\\"Semi-softcore, semi-pseudo action, semi-sci-fi film\\\" ... actually, I think I can say that this isn't even a film at all. If it wasn't for the main character's sheer hotness, my friend and I would've turned off the movie as the opening credits rolled.<br /><br />I have a few questions to the people (I wouldn't even dare say \\\"professionals\\\") who made this film. -One, In the future, why are they using the weaponry we used in 1999? Oh, wait, I get it, it's all that they could get their hands on,... right???... well then,.. why is the main character wearing what looks like a normal everyday linen shirt and a vest, kinda like what people wore in the late nineties? .... oh ... I get it ... in space, it MUST have been a fashion statement.... well, then... WHY,OH, WHY does the main character pull out a 3.5 floppy disk at one point in the film so that she can upload some bullshit ?! wtf !? ...we've progressed so far that we have space travel, but we still haven't progressed past 1.44megabytes of space..?<br /><br />I guess I'm just asking for too much.<br /><br />Question two, Let's just say...that yes... this is a softcore porn. Then why is there only ONE real sex scene, and why does it last for 2 minutes?<br /><br />I mean, you're taking the REASON people are staying up in the middle of the night to see this crap (dare I say 'movie' anymore?) ... and whittling it down to 2 minutes. Hell, they should've just taken that sex scene and sold it to another porn movie, and they would've STILL made more money off of this \\\"crap\\\" than they did.<br /><br />C.C.Cortigan is hot. And no offense to the actress, but she acts about as well as I do. and I'm mentally retarded, and only have one testicle... (C.C. Cortigan,...e-mail me ...we'll have lunch) I would write more, but I've run out of space.\"\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print test_data['review'][12]\n",
    "print result[12]\n",
    "\n",
    "print test_data['review'][78]\n",
    "print result[78]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "Distributed Word Vectors\n",
    "Word2vec, published by Google in 2013, is a neural network implementation that learns distributed representations for words. Word2Vec does not need labels in order to create meaningful representations. This is useful, since most data in the real world is unlabeled. If the network is given enough training data (tens of billions of words), it produces word vectors with intriguing characteristics. Words with similar meanings appear in clusters, and clusters are spaced such that some word relationships, such as analogies, can be reproduced using vector math.\n",
    "\n",
    "Using word vectors\n",
    "\n",
    "First, we read in the data with pandas, as we did above. But, we now use unlabeledTrain.tsv, which contains 50,000 additional reviews with no labels. When we built the Bag of Words model as above, extra unlabeled training reviews were not useful. However, since Word2Vec can learn from unlabeled data, these extra 50,000 reviews can now be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train_data = pd.read_csv(r\"C:\\Users\\52054867\\machine learning\\Word2Vec\\word2vec-nlp-tutorial\\unlabeledTrainData.tsv\\unlabeledTrainData.tsv\", delimiter=\"\\t\", header=0, quoting=3)\n",
    "#train_data =           pd.read_csv(r\"C:\\Users\\52054867\\machine learning\\Word2Vec\\word2vec-nlp-tutorial\\labeledTrainData.tsv\\labeledTrainData.tsv\", delimiter=\"\\t\", header=0)\n",
    "unlabeled_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'review'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\\\"Hey, let\\'s pool our money together and make a really bad movie!\\\\\" Or something like that. What ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc. All corners were cut, except the one that would have prevented this film\\'s release. Life\\'s like that.\"'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train_data['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "## break into sentences\n",
    "def review_to_sentences(review, tokenizer):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\52054867\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for review in unlabeled_train_data[\"review\"]:\n",
    "    sentences += review_to_sentences(unicode(review),tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528987"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'pathmark', u'means', u'savings']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[528986]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "print (\"Training Word2Vec model...\")\n",
    "model = Word2Vec(sentences, workers=num_workers, \\\n",
    "                 size=num_features, \\\n",
    "                 min_count = min_word_count,\\\n",
    "                 window = context, \\\n",
    "                 sample = downsampling,\\\n",
    "                 seed=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec_imdb_movie_review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec_imdb_movie_review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12907L, 300L)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_reviews = []\n",
    "for review in train_data[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec\n",
    "            featureVec = np.add(featureVec,model.wv.__getitem__([word]))\n",
    "\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    \n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        #\n",
    "        # Print a status message every 1000th review\n",
    "        if counter%1000. == 0.:\n",
    "            print counter\n",
    "        # \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        #\n",
    "        # Increment the counter\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000L, 300L)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs.dtype\n",
    "trainDataVecs_new = np.nan_to_num(trainDataVecs.astype(np.float64))\n",
    "trainDataVecs_new.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "forest = rf.fit(trainDataVecs_new,train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert test data to vetors\n",
    "clean_test_reviews = []\n",
    "for review in (test_data[\"review\"][0], test_data[\"review\"][1]):\n",
    "    clean_test_reviews.append( review_to_wordlist( review ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "print test_data[\"review\"][0]\n",
    "print result[0]\n",
    "print test_data[\"review\"][1]\n",
    "print result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
